{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PESTPP-IES - History Matching For \"Nothing\", Uncertainty Analysis for Free\n",
    "\n",
    "PESTPP-IES embodies an approach to data assimilation and uncertainty analysis which is a significant departure from the “calibrate first and do uncertainty analysis later” paradigm. PESTPP-IES does away with the search for a \"unique\" parameter field that \"calibrates\" a model. Instead, its goal is to obtain an _ensemble_ of parameter fields, all of which adequately reflect measured data and expert knowledge. \n",
    "\n",
    "Unlike PESTPP-GLM and PEST(\\_HP), PESTPP-IES does not calculate derivatives using finite parameter differences. Instead, it calculates approximate partial derivatives from cross-covariances between parameter values and model outputs that are calculated using members of the ensemble. A major benefit from this approach is that the number of model runs required to history-match an ensemble is independent of the number of adjustable parameters. This means that it is feasible for a model to employ a large number of adjustable parameters (history matching for \"nothing\"...). Conceptually, this reduces propensity for predictive bias at the same time as it protects against uncertainties of decision-critical model predictions being underestimated. \n",
    "\n",
    "PESTPP-IES commences by using a suite of random parameter fields sampled from the prior parameter probability distribution. Each parameter field is referred to as a “realisation”. The suite of realisations is referred to as an “ensemble”. Using an iterative procedure, PESTPP-IES modifies the parameters that comprise each realisation so that each is better able to better reflect historical data. In other words, PESTPP-IES adjusts the parameters of each realisation in order to reduce the misfit between simulated and measured observation values. \n",
    "\n",
    "But there is more! PESTPP-IES also accounts for the influence of observation noise. Each parameter realisation is (optionally) adjusted to fit a slightly different set of observation values, based on sampling the user-supplied variability assumed in the observation dataset. (An observation ensemble can be provided by the user or generated automatically by PESTPP-IES.) Thus, the uncertainty incurred through observation noise gets carried through to the predictions' posterior uncertainty. \n",
    "\n",
    "The outcome of this multi-realisation parameter adjustment process is an ensemble of parameter fields, all of which allow the model to adequately replicate observed system behaviour. These parameter fields can be considered samples of the posterior parameter probability distribution. By simulating a forecast with this ensemble of models, a sample of the _posterior forecast probability distribution_ is obtained (uncertainty analysis for free...). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Current Tutorial\n",
    "\n",
    "In the current notebook we are going to pick up after the [\"observation and weights\"](../part2_02_obs_and_weights/freyberg_obs_and_weights.ipynb) tutorial. We have prepared a high-dimensional PEST dataset and are ready to begin history matching. Here we are going to continue with the same PEST(++) control file, add some PESTPP-IES specific options and then run it.\n",
    "\n",
    "Then, we are going to take the opportunity to revisit some of the concepts of history matching induced bias. In subsequent tutorials, we will demonstrate several strategies to mitigate the potential for history matching to introduce bias into our forecasts.\n",
    "\n",
    "### Admin\n",
    "\n",
    "The modified Freyberg model is introduced in another tutorial notebook (see [\"freyberg intro to model\"](../part0_02_intro_to_freyberg_model/intro_freyberg_model.ipynb)). The current notebook picks up following the [\"observation and weights\"](../part2_02_obs_and_weights/freyberg_obs_and_weights.ipynb) notebook, in which a high-dimensional PEST dataset was constructed using `pyemu.PstFrom`. You may also wish to go through the \"intro to pyemu\" notebook beforehand.\n",
    "\n",
    "The next couple of cells load necessary dependencies and call a convenience function to prepare the PEST dataset folder for you. Simply press `shift+enter` to run the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt;\n",
    "import psutil\n",
    "\n",
    "import sys\n",
    "import pyemu\n",
    "import flopy\n",
    "assert \"dependencies\" in flopy.__file__\n",
    "assert \"dependencies\" in pyemu.__file__\n",
    "sys.path.insert(0,\"..\")\n",
    "import herebedragons as hbd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the template directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the temporary working folder\n",
    "t_d = os.path.join('freyberg6_template')\n",
    "# get the previously generated PEST dataset\n",
    "org_t_d = os.path.join(\"..\",\"part2_02_obs_and_weights\",\"freyberg6_template\")\n",
    "if not os.path.exists(org_t_d):\n",
    "    raise Exception(\"you need to run the '/part2_02_obs_and_weights/freyberg_obs_and_weights.ipynb' notebook\")\n",
    "if os.path.exists(t_d):\n",
    "    shutil.rmtree(t_d)\n",
    "shutil.copytree(org_t_d,t_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing for PESTPP-IES\n",
    "\n",
    "We shall start by running PESTPP-IES in standard, basic mode. \n",
    "\n",
    "First we need to load the existing control file and add some PESTPP-IES specific options. As with most PEST++ options, these mostly have pretty decent default values; however, depending on your setup you may wish to change them. We highly recommend reading the [PEST++ user manual](https://github.com/usgs/pestpp/blob/master/documentation/pestpp_users_manual.md) for full descriptions of the options and their default values.\n",
    "\n",
    "Load the PEST control file as a `Pst` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst_path = os.path.join(t_d, 'freyberg_mf6.pst')\n",
    "pst = pyemu.Pst(pst_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell is just in place to make sure you are running the tutorials in order. Check that we are at the right stage to run PESTPP-IES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if obs&weights notebook has been run\n",
    "if not pst.observation_data.observed.sum()>0:\n",
    "    raise Exception(\"You need to run the '/part2_02_obs_and_weights/freyberg_obs_and_weights.ipynb' notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick check of what PEST++ options are already in the control file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.pestpp_options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of realisations\n",
    "As previously mentioned, an “ensemble” refers to a set of parameter fields. Each of these parameter fields is referred to as a “realisation”. In general, the larger the ensemble, the better is the history-matching performance of PESTPP-IES. In practice, the number of realisations is limited by computing resources (more realisations require more model runs per iteration). \n",
    "\n",
    "The number of parameter fields comprising an ensemble should, at least, be larger than the dimensionality of the solution space of the inverse problem that is posed by the history-matching process. This ensures that the iterative ensemble smoother has access to the directions in parameter space that it requires in order to provide a good fit with the calibration dataset.\n",
    "\n",
    "Normally, this number can only be guessed. It is usually less than the number of observations comprising a calibration dataset, and can never be greater than this. Furthermore, the dimensionality of the solution space decreases with the amount of noise that accompanies the dataset. If a Jacobian matrix is available (it may remain from a previous calibration exercise), pyEMU (or the SUPCALC utility from the PEST suite) can be used to assess solution space dimensionality. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this tutorial, lets just use a few realizations to speed things up (feel free to use less or more - choose your own adventure!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.nnz_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.pestpp_options[\"ies_num_reals\"] = 30  # starting with a real small ensemble!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note: even though the prior parameter ensemble stored in `prior_pe.jcb` has many more realizations, PESTPP-IES will truncate this ensemble to the `ies_num_reals` size if `ies_num_reals` is less than the number of realizations in the user-supplied prior parameter ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior Parameter Ensemble\n",
    "\n",
    "As already mentioned, PESTPP-IES starts from an ensemble of parameter realisations. You can provide an ensemble yourself. If you do _not_ provide a prior parameter ensemble, PESTPP-IES will generate one itself by sampling parameter values from a multi-Gaussian distribution. In doing so, it will assume that parameter values listed in the control file \"parameter data\" section reflect the mean of the distribution. If no other information is provided, PESTPP-IES will calculate the standard deviation assuming that parameter upper and lower bounds reflect the 95% confidence interval, and that all parameters are statistically independent.\n",
    "\n",
    "Now, we are all sophisticated people that recognize the importance of heterogeneity and spatial (and temporal) correlation between parameters. So, as a third alternative, we can inform PESTPP-IES of these covariances by providing a covariance matrix to the pest++ control variable `parcov()`. We prepared a geostatistical prior parameter covariance matrix during the \"pstfrom pest setup\" tutorial (the file named `prior_cov.jcb`). \n",
    "\n",
    "Alternatively, we can provide PESTPP-IES with a pre-prepared ensemble of parameter realisations. We also prepared one during the [\"pstfrom pest setup\"](../part2_01_pstfrom_pest_setup/freyberg_pstfrom_pest_setup.ipynb) tutorial and recorded it in binary format. It is the file named `prior_pe.jcb`.\n",
    "\n",
    "Load the prior parameter ensemble we generated previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f for f in os.listdir(t_d) if f.endswith(\".jcb\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = pyemu.ParameterEnsemble.from_binary(pst=pst,filename=os.path.join(t_d,\"prior_pe.jcb\"))\n",
    "pe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK then. We need to tell PESTPP-IES which file to use. Easy enough, just assign the file name to the `ies_parameter_ensemble()` pestpp control variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.pestpp_options['ies_parameter_ensemble'] = 'prior_pe.jcb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation plus noise Ensemble\n",
    "\n",
    "We also can also include an ensemble of observation plus noise realizations; these realizations allow measurement noise to be incorporated into the posterior parameter (and therefore forecast) ensemble.  Conceptually, each parameter realization in the prior ensemble is paired with a realization of observed values plus measurement noise.  This means that each parameter realization is seeking to match a (slightly) different set of observed values.  This is the reason pestpp-ies report a \"measured\" objective function summary and an \"actual\" objective function summary; the former includes the noise realizations, the latter does not.\n",
    "\n",
    "As for parameters, PESTPP-IES will, by default, generate noise realizations from `standard_deviation` values in the control file that we added back in the \"obs and weights\" notebook, and that noise has a normal distribution. The observation ensemble is generated by adding the ensemble of noise to the observation target values in the control file. (The \"obs and weights\" tutorial notebook covers these topics in more detail.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right then, let's do a pre-flight check to make sure everything is working. It's always good to do the 'ole `noptmax=0` test.  However PESTPP-IES has an even more advanced checking option: `noptmax=-2`, which triggers PESTPP-IES to load and/or draw all ensemble components and then run the mean parameter ensemble vector.  So still a single model run, but this requires PESTPP-IES to initialize more of the components that will be eventually used for data assimilation, so its a more rigorous check of the inputs provided. Set NOPTMAX to minus 2 and run PEST++IES once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.control_data.noptmax = -2\n",
    "pst.write(os.path.join(t_d, 'freyberg_mf6.pst'),version=2)\n",
    "\n",
    "pyemu.os_utils.run(\"pestpp-ies freyberg_mf6.pst\",cwd=t_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that was sucessfull, we can re-load it and just check the Phi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pyemu.Pst(os.path.join(t_d, 'freyberg_mf6.pst'))\n",
    "pst.phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other options that are usually a good idea\n",
    "\n",
    "PESTPP-IES has heaps of options that can be brought to bare for any given problem but most of these have good default values.  However, two options that can be very important for highly nonlinear inverse problems are related to how and when to give up on realizations that are just not doing very well with respect to phi.  These lagging realizations can dominate the objective function and therefore influence the choice of lambda values, and, ultimately, the success of the PESTPP-IES analysis. These two options are `ies_bad_phi` and `ies_bad_phi_sigma`.  The first one, `ies_bad_phi` is an absolute tolerance on the maximum phi value that will be allowed for any realization in the ensemble at any point in the algorithm.  If a realization yields a phi greater than `ies_bad_phi`, it will be \"dropped\" from the ensemble.  This can be an important option if cells go dry and yield an enormous phi.  \n",
    "\n",
    "The second option, `ies_bad_phi_sigma` is a relative tolerance on the phi of a realization vs the mean phi of all the realizations in the ensemble.  The \"sigma\" tells us standard deviation is involved in this option:  `ies_bad_phi_sigma` is the standard deviation \"distance\" from the mean phi that realizations are allowed to have before they are dropped.  For example, if the mean phi is 100 and standard deviation around this mean is 20, and `ies_bad_phi_sigma` = 2, then any realization with a phi greater than 100 + (2 * 20) = 140 is dropped.  This \"adaptive\" rejection filter adapts as the ensemble evolves across iterations, and, for highly nonlinear problems, this option can make a huge difference.  Values between 1.5 (more aggressive) and 2.5 (more tolerant) seem to work well..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.pestpp_options[\"ies_bad_phi_sigma\"] = 2.0 #middle ground value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run PESTPP-IES\n",
    "\n",
    "Right then, let's do this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update NOPTMAX again and re-write the control file\n",
    "pst.control_data.noptmax = 3\n",
    "pst.write(os.path.join(t_d, 'freyberg_mf6.pst'),version=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up the process, you will want to distribute the workload across as many parallel agents as possible. Normally, you will want to use the same number of agents (or less) as you have available CPU cores. Most personal computers (i.e. desktops or laptops) these days have between 4 and 10 cores. Servers or HPCs may have many more cores than this. Another limitation to keep in mind is the read/write speed of your machines disk (e.g. your hard drive). PEST and the model software are going to be reading and writing lots of files. This often slows things down if agents are competing for the same resources to read/write to disk.\n",
    "\n",
    "The first thing we will do is specify the number of agents we are going to use.\n",
    "\n",
    "__Attention!__\n",
    "\n",
    "You must specify the number which is adequate for ***your*** machine! Make sure to assign an appropriate value for the following `num_workers` variable - if its too large for your machine, #badtimes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the number of physical cores avalable on your machine using `psutils`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psutil.cpu_count(logical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 14 #update this according to your resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we shall specify the PEST run-manager/master directory folder as `m_d`. This is where outcomes of the PEST run will be recorded. It should be different from the `t_d` folder, which contains the \"template\" of the PEST dataset. This keeps everything separate and avoids silly mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_d = os.path.join('master_ies_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell deploys the PEST agents and manager and then starts the run using PESTPP-IES. Run it by pressing `shift+enter`. If you wish to see the outputs in real-time, switch over to the terminal window (the one which you used to launch the `jupyter notebook`). There you should see PESTPP-IES's progress. \n",
    "\n",
    "If you open the tutorial folder, you should also see a bunch of new folders there named `worker_0`, `worker_1`, etc. These are the agent folders. The `master_ies` folder is where the manager is running. \n",
    "\n",
    "This run should take several minutes to complete (depending on the number of workers and the speed of your machine). If you get an error, make sure that your firewall or antivirus software is not blocking PESTPP-IES from communicating with the agents (this is a common problem!).\n",
    "\n",
    "> **Pro Tip**: Running PEST from within a `jupyter notebook` has a tendency to slow things down and hog alot of RAM. When modelling in the \"real world\" it is often more efficient to implement workflows in scripts which you can call from the command line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.os_utils.start_workers(t_d, # the folder which contains the \"template\" PEST dataset\n",
    "                            'pestpp-ies', #the PEST software version we want to run\n",
    "                            'freyberg_mf6.pst', # the control file to use with PEST\n",
    "                            num_workers=num_workers, #how many agents to deploy\n",
    "                            worker_root='.', #where to deploy the agent directories; relative to where python is running\n",
    "                            master_dir=m_d, #the manager directory\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Outcomes\n",
    "\n",
    "Right then. PESTPP-IES completed successfully. Let's take a look at some of the outcomes.\n",
    "\n",
    "First lets open the \"freyberg_mf6.rec\" in the `master_ies_1` directory and make sure we understand what is going on.  Make sure to look for :\n",
    " - initial phi summary\n",
    " - number of runs per iteration\n",
    " - number of active realizations per iteration\n",
    " - parameter change summary per iteration\n",
    " \n",
    "Do you understand the flow of the algorithm? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is usually useful to take a look at how the ensemble performed overall at fitting historical data and how fit evolved at each PESTPP-IES iteration. Let's make a cheap Phi progress plot. PESTPP-IES recorded the Phi ($\\Phi$) for each iteration in the file named `freyberg_mf6.phi.actual.csv`. Here you will find a summary of the ensembles' Phi, as well as the Phi from each individual realization. Note that the Phi in this file is calculated from the residual between simulated values and the observation values in the _control file_ (i.e. the measured data). These differ from values in `freyberg_mf6.phi.meas.csv`, which records Phi values calculated from the residual between simulated values and observation values + realizations of noise.  \n",
    "\n",
    "Let's make a plot of the Phi progress from each realization against the total number of model runs.\n",
    "\n",
    "Wow! Check that out. With a measly few hundred model runs we have a pretty decent fit for most of the ensemble. (How does this compare to the fit achieved with derivative-based methods in the \"glm part 2\" tutorial?) Recall here we are using many thousand parameters...pretty amazing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, sharey=True, figsize=(10,3.5))\n",
    "# left\n",
    "ax = axes[0]\n",
    "phi = pd.read_csv(os.path.join(m_d,\"freyberg_mf6.phi.actual.csv\"),index_col=0)\n",
    "phi.index = phi.total_runs\n",
    "phi.iloc[:,6:].apply(np.log10).plot(legend=False,lw=0.5,color='k', ax=ax)\n",
    "ax.set_title(r'Actual $\\Phi$')\n",
    "ax.set_ylabel(r'log $\\Phi$')\n",
    "# right\n",
    "ax = axes[-1]\n",
    "phi = pd.read_csv(os.path.join(m_d,\"freyberg_mf6.phi.meas.csv\"),index_col=0)\n",
    "phi.index = phi.total_runs\n",
    "phi.iloc[:,6:].apply(np.log10).plot(legend=False,lw=0.2,color='r', ax=ax)\n",
    "ax.set_title(r'Measured+Noise $\\Phi$')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whilst we are at it, why not plot a histogram of Phi from the last iteration. Good to get a depiction of how Phi is distributed across the ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "phi.iloc[-1,6:].hist()\n",
    "plt.title(r'Final $\\Phi$ Distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PESTPP-IES has conveniently kept track of all our observation data, measurement noise and the model outputs from each realization at each iteration. This now allows us to go back and look at all this information in detail should we wish to do so. We are interested in looking at (1) how model outputs compare to measured data+noise and (2) the distribution of model outputs for forecast observations.\n",
    "\n",
    "Since PESTPP-IES evaluates a prior parameter ensemble, we can use the model outputs from that iteration (iteration zero) as a sample of the prior. We treat the model outputs from the ensemble for the final (best?) iteration as a sample of the posterior. Let's read in the files which PESTPP-IES recorded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_oe = pyemu.ObservationEnsemble.from_csv(pst=pst,filename=os.path.join(m_d,\"freyberg_mf6.0.obs.csv\"))\n",
    "pt_oe = pyemu.ObservationEnsemble.from_csv(pst=pst,filename=os.path.join(m_d,\"freyberg_mf6.{0}.obs.csv\".format(pst.control_data.noptmax)))\n",
    "noise = pyemu.ObservationEnsemble.from_csv(pst=pst,filename=os.path.join(m_d,\"freyberg_mf6.obs+noise.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a quick comparison of Phi between the prior and posterior. Nice improvement overall!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1)\n",
    "pr_oe.phi_vector.apply(np.log10).hist(ax=ax,fc=\"0.5\",ec=\"none\",alpha=0.5,density=False)\n",
    "pt_oe.phi_vector.apply(np.log10).hist(ax=ax,fc=\"b\",ec=\"none\",alpha=0.5,density=False)\n",
    "_ = ax.set_xlabel(\"$log_{10}\\\\phi$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, what we really want to see. Let's plot timeseries of simulated versus measured observation values. We are going to do this many times in this notebook, so let's make a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tseries_ensembles(pr_oe, pt_oe, noise, onames=[\"hds\",\"sfr\"]):\n",
    "    pst.try_parse_name_metadata()\n",
    "    # get the observation data from the control file and select \n",
    "    obs = pst.observation_data.copy()\n",
    "    # onames provided in oname argument\n",
    "    obs = obs.loc[obs.oname.apply(lambda x: x in onames)]\n",
    "    # only non-zero observations\n",
    "    obs = obs.loc[obs.obgnme.apply(lambda x: x in pst.nnz_obs_groups),:]\n",
    "    # make a plot\n",
    "    ogs = obs.obgnme.unique()\n",
    "    fig,axes = plt.subplots(len(ogs),1,figsize=(10,2*len(ogs)))\n",
    "    ogs.sort()\n",
    "    # for each observation group (i.e. timeseries)\n",
    "    for ax,og in zip(axes,ogs):\n",
    "        # get values for x axis\n",
    "        oobs = obs.loc[obs.obgnme==og,:].copy()\n",
    "        oobs.loc[:,\"time\"] = oobs.time.astype(float)\n",
    "        oobs.sort_values(by=\"time\",inplace=True)\n",
    "        tvals = oobs.time.values\n",
    "        onames = oobs.obsnme.values\n",
    "        # plot prior\n",
    "        [ax.plot(tvals,pr_oe.loc[i,onames].values,\"0.5\",lw=0.5,alpha=0.5) for i in pr_oe.index]\n",
    "        # plot posterior\n",
    "        [ax.plot(tvals,pt_oe.loc[i,onames].values,\"b\",lw=0.5,alpha=0.5) for i in pt_oe.index]\n",
    "        # plot measured+noise \n",
    "        oobs = oobs.loc[oobs.weight>0,:]\n",
    "        tvals = oobs.time.values\n",
    "        onames = oobs.obsnme.values\n",
    "        [ax.plot(tvals,noise.loc[i,onames].values,\"r\",lw=0.5,alpha=0.5) for i in noise.index]\n",
    "        ax.plot(oobs.time,oobs.obsval,\"r-\",lw=2)\n",
    "        ax.set_title(og,loc=\"left\")\n",
    "    fig.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now plot the time series of non-zero observation groups. Let's just plot the timeseries of absolute values of heads and stream gage flow.\n",
    "\n",
    "In the plots below, light grey lines are timeseries simulated with the prior parameter ensemble. Blue lines are model outputs simulated using the posterior parameter ensemble. Red lines are the ensemble of measurement + noise. \n",
    "\n",
    "Looks like we are getting an excellent fit. All the blue lines are within the same areas as the red lines. This implies we have achieved a level of fit commensurate with measurement noise. Sounds very positive. What do you think? Success? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_tseries_ensembles(pr_oe, pt_oe, noise, onames=[\"hds\",\"sfr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Some additional filtering\n",
    "\n",
    "Often a few realizations perform particularly poorly. In such cases it can be good practice to remove them. Easy enough to do. For example, the cell below drops any realizations that did not achieve a Phi lower than the  threshold value assigned to the variable `thresh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold Phi\n",
    "thres = phi.iloc[-1,6:].quantile(0.9)\n",
    "# drop reals with Phi > thresh\n",
    "pv = pt_oe.phi_vector\n",
    "keep = pv.loc[pv<thres]\n",
    "if keep.shape[0] != pv.shape[0]:\n",
    "    print(\"reducing posterior ensemble from {0} to {1} realizations\".format(pv.shape[0],keep.shape[0]))\n",
    "    pt_oe = pt_oe.loc[keep.index,:]\n",
    "    fig,ax = plt.subplots(1,1)\n",
    "    pr_oe.phi_vector.apply(np.log10).hist(ax=ax,fc=\"0.5\",ec=\"none\",alpha=0.5,density=False)\n",
    "    pt_oe.phi_vector.apply(np.log10).hist(ax=ax,fc=\"b\",ec=\"none\",alpha=0.5,density=False)\n",
    "    _ = ax.set_xlabel(\"$log_{10}\\\\phi$\")\n",
    "else:\n",
    "    print('No realizations dropped.')\n",
    "if pt_oe.shape[0] == 0:\n",
    "    print(\"filtered out all posterior realization #sad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "lets visualize the HK array, including what is called the \"base\" realization (aka the minimum error variance parameter values) - these are the closest parameter values to what pestpp-glm/pest_hp would give you.  In fact, if you are only going to use one realization from pestpp-ies for additional analyses, you should always and forever use this parameter set - do not ever-EVER- just pick the realization with the lowest phi (this is tempting because we are all groundwater modelers and therefore are obsessed with minimum phi!).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = flopy.mf6.MFSimulation.load(sim_ws=m_d)\n",
    "ib = sim.get_model().dis.idomain.array[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hk(pr_oe, pt_oe, pst):\n",
    "    obs = pst.observation_data\n",
    "    hkobs = obs.loc[obs.oname==\"hk\",:].copy()\n",
    "    hkobs[\"i\"] = hkobs.i.astype(int)\n",
    "    hkobs[\"j\"] = hkobs.j.astype(int)\n",
    "    real = pt_oe.index[0]\n",
    "    \n",
    "    fig,axes = plt.subplots(2,4,figsize=(15,10))\n",
    "    prmn,prstd,prmev,prreal = np.zeros_like(ib,dtype=float),np.zeros_like(ib,dtype=float),np.zeros_like(ib,dtype=float),np.zeros_like(ib,dtype=float)\n",
    "    prmn[hkobs.i,hkobs.j] = pr_oe._df.loc[:,hkobs.obsnme].mean()\n",
    "    prstd[hkobs.i,hkobs.j] = pr_oe._df.loc[:,hkobs.obsnme].std()\n",
    "    prmev[hkobs.i,hkobs.j] = pr_oe.loc[\"base\",hkobs.obsnme].values   \n",
    "    prreal[hkobs.i,hkobs.j] = pr_oe.loc[real,hkobs.obsnme].values   \n",
    "    ptmn,ptstd,ptmev,ptreal = np.zeros_like(ib,dtype=float),np.zeros_like(ib,dtype=float),np.zeros_like(ib,dtype=float),np.zeros_like(ib,dtype=float)\n",
    "    ptmn[hkobs.i,hkobs.j] = pt_oe._df.loc[:,hkobs.obsnme].mean()\n",
    "    ptstd[hkobs.i,hkobs.j] = pt_oe._df.loc[:,hkobs.obsnme].std()\n",
    "    ptmev[hkobs.i,hkobs.j] = pt_oe.loc[\"base\",hkobs.obsnme].values  \n",
    "    ptreal[hkobs.i,hkobs.j] = pt_oe.loc[real,hkobs.obsnme].values   \n",
    "    for arr in [prmn,prstd,prmev,prreal,ptmn,ptstd,ptmev,ptreal]:\n",
    "        #arr = np.log10(arr)\n",
    "        arr[ib>0] = np.log10(arr[ib>0])\n",
    "        arr[ib==0] = np.nan\n",
    "    prarrs = [prmn,prstd,prmev,prreal]\n",
    "    ptarrs = [ptmn,ptstd,ptmev,ptreal]\n",
    "    titles = [\"mean\",\"stdev\",\"MEV\",\"realization {0}\".format(real)]\n",
    "    \n",
    "    for pr,pt,axes,title in zip(prarrs,ptarrs,axes.transpose(),titles):\n",
    "        vmn,vmx = min(np.nanmin(pr),np.nanmin(pt)),max(np.nanmax(pr),np.nanmax(pt))\n",
    "        cb = axes[0].imshow(pr,vmin=vmn,vmax=vmx)\n",
    "        plt.colorbar(cb,ax=axes[0],label=\"$log_{10}\\\\frac{m}{d}$\")\n",
    "        axes[0].set_title(\"prior \"+title)\n",
    "        cb = axes[1].imshow(pt,vmin=vmn,vmax=vmx)\n",
    "        plt.colorbar(cb,ax=axes[1],label=\"$log_{10}\\\\frac{m}{d}$\")\n",
    "        axes[1].set_title(\"posterior \"+title)\n",
    "    plt.tight_layout()\n",
    "    return fig,axes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_ = plot_hk(pr_oe,pt_oe, pst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the MEV posterior HK field is pretty noisy - conceptually we would expect it to be much smoother than the stochastic realizations (last column)...spoiler alert:  its because we used only 30 realization so there is a lot of so-called \"spurious correlation\", which is essential bogus relations between parameters and observations. These bogus relations happen because we only have 30 pairs of numbers to calculate correlation coefficients with, and the result of these bogus relations is excessively noisey parameter fields and under-estimation of posterior parameter and forecast uncertainty..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasts\n",
    "\n",
    "As usual, we bring this story back to the forecasts - after all they are why we are modeling. As this is a synthetic case and we know the \"truth\", we have benefit of being able to check the reliability of our forecast. Let's do that now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick reminder of the observations that record our forecast value of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.forecast_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to plot histograms of the forecast values simulated by the model using parameters from the (1) prior and (2) posterior ensembles. Simulated forecast values are recorded in the observation ensembles we read in earlier (as are all observations listed in the PEST control file).\n",
    "\n",
    "Again, we are going to do this a lot in the current and subsequent tutorials, so let's just make a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_forecast_hist_compare(pt_oe,pr_oe, last_pt_oe=None,last_prior=None ):\n",
    "        num_plots = len(pst.forecast_names)\n",
    "        num_cols = 1\n",
    "        if last_pt_oe!=None:\n",
    "            num_cols=2\n",
    "        fig,axes = plt.subplots(num_plots, num_cols, figsize=(5*num_cols,num_plots * 2.5), sharex='row',sharey='row')\n",
    "        for axs,forecast in zip(axes, pst.forecast_names):\n",
    "            # plot first column with currrent outcomes\n",
    "            if num_cols==1:\n",
    "                axs=[axs]\n",
    "            ax = axs[0]\n",
    "            # just for aesthetics\n",
    "            bin_cols = [pt_oe.loc[:,forecast], pr_oe.loc[:,forecast],]\n",
    "            if num_cols>1:\n",
    "                bin_cols.extend([last_pt_oe.loc[:,forecast],last_prior.loc[:,forecast]])\n",
    "            bins=np.histogram(pd.concat(bin_cols),\n",
    "                                         bins=20)[1] #get the bin edges\n",
    "            pr_oe.loc[:,forecast].hist(facecolor=\"0.5\",alpha=0.5, bins=bins, ax=ax)\n",
    "            pt_oe.loc[:,forecast].hist(facecolor=\"b\",alpha=0.5, bins=bins, ax=ax)\n",
    "            ax.set_title(forecast)\n",
    "            fval = pst.observation_data.loc[forecast,\"obsval\"]\n",
    "            ax.plot([fval,fval],ax.get_ylim(),\"r-\")\n",
    "            # plot second column with other outcomes\n",
    "            if num_cols >1:\n",
    "                ax = axs[1]\n",
    "                last_prior.loc[:,forecast].hist(facecolor=\"0.5\",alpha=0.5, bins=bins, ax=ax)\n",
    "                last_pt_oe.loc[:,forecast].hist(facecolor=\"b\",alpha=0.5, bins=bins, ax=ax)\n",
    "                ax.set_title(forecast)\n",
    "                fval = pst.observation_data.loc[forecast,\"obsval\"]\n",
    "                ax.plot([fval,fval],ax.get_ylim(),\"r-\")\n",
    "        # set ax column titles\n",
    "        if num_cols >1:\n",
    "            axes.flatten()[0].text(0.5,1.2,\"Current Attempt\", transform=axes.flatten()[0].transAxes, weight='bold', fontsize=12, horizontalalignment='center')\n",
    "            axes.flatten()[1].text(0.5,1.2,\"Previous Attempt\", transform=axes.flatten()[1].transAxes, weight='bold', fontsize=12, horizontalalignment='center')\n",
    "        fig.tight_layout()\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the forecast histograms. Grey columns are the prior. Blue columns are the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_forecast_hist_compare(pt_oe=pt_oe, pr_oe=pr_oe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ruh roh!  The posterior is very narrow for all forecasts...isn't covering the correct values (with lots of realizations) for some. Here is the result of those \"spurious correlations\"... #badtimes. \n",
    "\n",
    "But hold on! The prior does (nearly) cover the true values for all forecasts and is much wider. So that implies there is somewhere between the prior and posterior we have now, which is optimal with respect to all forecasts. Hmmm...so this means that history matching to a high level of fit, with these relatively few realisations, made our prediction worse. We have incurred forecast-sensitive bias through the parameter adjustment process. How can we fit historical data so well but get the \"wrong\" answer for some of the forecasts?\n",
    "\n",
    "> **Important Aside**! When you are using an imperfect model (compared to the truth), the link between a \"good fit\" and robust forecast is broken. A good fit does not mean a good forecaster! This is particularly the case for forecasts that are sensitive to combinations of parameters that occupy the history-matching null space (see [Doherty and Moore (2020)](https://s3.amazonaws.com/docs.pesthomepage.org/documents/model_complexity_monograph.pdf) for a discussion of these concepts). In other words, forecasts which rely on (combinations of) parameters that are not informed by available observation data. (In our case, an example is the \"headwater\" forecast.)\n",
    "\n",
    "### Underfitting\n",
    "So, somewhere between the prior and the final iteration is the optimal amount of parameter adjustment that (1) reduces uncertainty some (but not too much - subjective again...) but (2) does not incur forecast bias. We saw that the posterior for our last iteration achieved a level of fit commensurate with measurement error. So we achieved as good a fit as could be expected with the data. But now we have seen that getting that fit incurred bias. So what happens if we \"underfit\"? i.e. accept a level of fit which is _worse_ than can be explained by noise in the _measured_ data.\n",
    "\n",
    "Luckily, we can just load up a previous iteration of PESTPP-IES results and use those! Let's see if that resolves our predicament.\n",
    "\n",
    "Check the outcomes from the first iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_to_use_as_posterior = 1\n",
    "pt_oe_iter = pyemu.ObservationEnsemble.from_csv(pst=pst,filename=os.path.join(m_d,\"freyberg_mf6.{0}.obs.csv\".\\\n",
    "                                                                         format(iter_to_use_as_posterior)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1)\n",
    "pr_oe.phi_vector.apply(np.log10).hist(ax=ax,fc=\"0.5\",ec=\"none\",alpha=0.5,density=False)\n",
    "pt_oe_iter.phi_vector.apply(np.log10).hist(ax=ax,fc=\"b\",ec=\"none\",alpha=0.5,density=False)\n",
    "_ = ax.set_xlabel(r\"$log_{10}\\phi$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, as you'd expect, some improvement in Phi, but not as much as for the final iteration:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at those time series again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_tseries_ensembles(pr_oe, pt_oe_iter,noise, onames=[\"hds\",\"sfr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go...much less satisfying. Clearly not \"as good\" a replica of observed behaviour. We also see more variance in the simulated equivalents (blue lines) to the observations, meaning we aren't fitting the historic observations as well...basically, we have only eliminated the extreme prior realizations - we can call this \"light\" conditioning or \"underfitting\"..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_ = plot_hk(pr_oe,pt_oe_iter, pst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the parameters, we can see that iteration 1 has substantially higher posterior variance than the previous plots - this is good sign!\n",
    "\n",
    "Finally, let's see what has happened to the forecasts. The next cell will plot the forecast histograms from the current \"posterior\" (right column of plots), alongside those from the previous attempt (left column of plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_forecast_hist_compare(pt_oe=pt_oe_iter,pr_oe=pr_oe,\n",
    "                                last_pt_oe=pt_oe,last_prior=pr_oe\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! That looks (mostly) better for our forecasts. What have we done? We've accepted \"more uncertainty\" for a reduced potential for inducing forecast bias.   This is an extremely important concept is applied real-world groundwater modeling: purposefully under-fitting observations has been shown (both theorically and empirically) to yield less bias in important predictive outcomes...but if we are too cautious (just using the prior), the predictive uncertainty might be so large that the modeling results are useless to stakeholders...#modelingistough\n",
    "\n",
    "But even if we hadn't failed to capture the truth with the final iteration results, in the real-world how would we know? So...should we just stick with prior? (Assuming the prior is adequately described...) Feeling depressed yet? Worry not, in our next tutorial we will introduce some coping strategies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another idea -- more realizations?\n",
    "Perhaps if we had more realizations we would have gotten a wider sample of the posterior? Maybe we could more effectively reduce our forecast uncertainty without incurring this nasty bias?\n",
    "\n",
    "Let's take a look at that..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take this from 30 -> 150, 250, 350? realizations (we have up to 1,000 to play with in our ensemble draw!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.pestpp_options[\"ies_num_reals\"] = 500  # have a play with this number. \n",
    "m_d = \"master_ies_1a\"\n",
    "pst.write(os.path.join(t_d,\"freyberg_mf6.pst\"),version=2)\n",
    "pyemu.os_utils.start_workers(t_d, # the folder which contains the \"template\" PEST dataset\n",
    "                            'pestpp-ies', #the PEST software version we want to run\n",
    "                            'freyberg_mf6.pst', # the control file to use with PEST\n",
    "                            num_workers=num_workers, #how many agents to deploy\n",
    "                            worker_root='.', #where to deploy the agent directories; relative to where python is running\n",
    "                            master_dir=m_d, #the manager directory\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, sharey=True, figsize=(10,3.5))\n",
    "# left\n",
    "ax = axes[0]\n",
    "phi = pd.read_csv(os.path.join(m_d,\"freyberg_mf6.phi.actual.csv\"),index_col=0)\n",
    "phi.index = phi.total_runs\n",
    "phi.iloc[:,6:].apply(np.log10).plot(legend=False,lw=0.5,color='k', ax=ax)\n",
    "ax.set_title(r'Actual $\\Phi$')\n",
    "ax.set_ylabel(r'log $\\Phi$')\n",
    "# right\n",
    "ax = axes[-1]\n",
    "phi = pd.read_csv(os.path.join(m_d,\"freyberg_mf6.phi.meas.csv\"),index_col=0)\n",
    "phi.index = phi.total_runs\n",
    "phi.iloc[:,6:].apply(np.log10).plot(legend=False,lw=0.2,color='r', ax=ax)\n",
    "ax.set_title(r'Measured+Noise $\\Phi$')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_oe_more = pyemu.ObservationEnsemble.from_csv(pst=pst,filename=os.path.join(m_d,\"freyberg_mf6.0.obs.csv\"))\n",
    "pt_oe_more = pyemu.ObservationEnsemble.from_csv(pst=pst,filename=os.path.join(m_d,\"freyberg_mf6.{0}.obs.csv\".format(pst.control_data.noptmax)))\n",
    "noise_more = pyemu.ObservationEnsemble.from_csv(pst=pst,filename=os.path.join(m_d,\"freyberg_mf6.obs+noise.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_hk(pr_oe_more,pt_oe_more, pst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! Now we are getting somewhere.  Depending on how many realizations you used, the posterior MEV should be (much) smoother than previously-more like we expected it...and the posterior standard deviation should (much) higher than previously...let's see if those good times continue when we check the forecasts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_forecast_hist_compare(pt_oe=pt_oe_more,pr_oe=pr_oe_more,\n",
    "                                last_pt_oe=pt_oe_iter,last_prior=pr_oe\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at that! Forecast distributions for a larger ensemble after 3 iterations (left) and a tiny ensemble (30) after just 1 iteration. How different do these distributions look? Perhaps not as different as we might hope for the extra model runs!\n",
    "However, having more realizations has two benefits:  more samples for uncertainty analysis and better resolution of the empirical first-order relations between parameters and observations - avoiding those bogus relations.  In this case, these two combined effects have helped us better bracket the true value for each forecast - yeh!  So always use a many realizations as you can tolerate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Question: How do we do better with ensemble methods?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Answer: use more realizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In summary\n",
    "We have learned:\n",
    " - How to configure and run PESTPP-IES\n",
    " - How to explore some of the outcomes\n",
    " - __Most importantly__, we have learned that getting a good fit does not a good predictor make. Trying to fit measured data with an imperfect model (which every model is...) can induce bias. \n",
    "\n",
    "If prior uncertainty is sufficient for decision-support purposes, it may be more robust to forgo history matching entirely. However, if uncertainty reduction is required, additional strategies to avoid inducing bias are needed. We will address some of these in subsequent tutorials."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
