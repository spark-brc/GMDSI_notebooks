{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt;\n",
    "import psutil\n",
    "\n",
    "import sys\n",
    "import pyemu\n",
    "import flopy\n",
    "assert \"dependencies\" in flopy.__file__\n",
    "assert \"dependencies\" in pyemu.__file__\n",
    "sys.path.insert(0,\"..\")\n",
    "import herebedragons as hbd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-space Inversion\n",
    "\n",
    "Data space inversion (DSI) enables the exploration of a model prediction's posterior distribution without requiring the exploration of the posterior distribution of model parameters. This is achieved by constructing a surrogate model using principal component analysis (PCA) of a covariance matrix of model outputs. This matrix links model outputs corresponding to field measurements with predictions of interest. The resulting predictions are then conditioned on real-world measurements of system behavior in the latent PCA subspace - whew!\n",
    "\n",
    "DSI can be an efficient tool for predictive uncertainty quantification, as it allows for the exploration of the uncertainty in model predictions without the need to explore the uncertainty in model parameters. This is particularly useful when the model is complex and the parameter space is high-dimensional. DSI can also be used to explore the sensitivity of model predictions to different types of data, and to identify the most informative data types for reducing predictive uncertainty. \n",
    "\n",
    "See the \"intro to EVA and DSI\" notebook for a more detailed explanation of the method, as well as the original papers [Sun and Durlofsky (2017)](https://doi.org/10.1007/s11004-016-9672-8), and subsequent variations (e.g., [Lima et al 2020](https://doi.org/10.1007/s10596-020-09933-w)). There are also [GMDSI webinars and lectures available on YouTube](https://www.youtube.com/watch?v=s2g3HaJa1Wk&t=1564s).\n",
    "\n",
    "In these notebooks we will focus on how to implement DSI for predictive uncertainty quantification, using pyEMU and PEST++. We will not discuss the maths behind the method in detail, but rather focus on how to use the tools to implement it. For a review of the maths see the [\"intro to eva and dsi\"](../part0_intro_to_dsi/intro_to_dsi.ipynb) notebook.\n",
    "\n",
    "## Getting ready\n",
    "\n",
    "Undertaking DSI relies on the existence of an ensemble of model-generated outputs (i.e., observations in the pest control file) for both historical observation quantities (eg heads, flows, concentrations, etc) AND forecast quantities of interest - this is important so we will say it again: DSI requires the results of a Monte Carlo set of runs for both historic and future/scenario (prediction) conditions. These results are generated by running the model with a range of parameter values, which are usually sampled from the prior parameter distribution. Note that this distribution does not need to be Gaussian, and each model \"parameterization\" can be as complex as the user desires. Generating the the combined historic-future/scenario output ensemble is the only time that the numerical model needs to be run. Ideally, the ensemble size should be as large as you can afford. However, once generated, the DSI data-driven/emulator \"model\" runs very quickly.\n",
    "\n",
    "Let us start by generating the ensemble of model outputs. We will make use of the prior ensemble generated in a previous tutorial. The next couple of cells load necessary dependencies and call a convenience function to prepare the PEST dataset folder for you. This is the same dataset that was constructed during the [\"freyberg ies\"](../part2_06_ies/freyberg_ies_1_basics.ipynb) tutorial. Simply press `shift+enter` to run the cells.\n",
    "\n",
    "Note: in our tutorial case, there is not actually much computational advantage in using the emulator (i.e., DSI) versus using the numerical model (i.e. Freyberg model). This is because the Freyberg model is very fast already. However, the DSI method is very useful for more computationally expensive models.\n",
    "\n",
    "..anyway...lez'go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the template directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the temporary working folder\n",
    "ies_d = os.path.join('master_ies_1a')\n",
    "if os.path.exists(ies_d):\n",
    "    shutil.rmtree(ies_d)\n",
    "\n",
    "org_t_d = os.path.join(\"..\",\"part2_06_ies\",\"master_ies_1a\")\n",
    "if not os.path.exists(org_t_d):\n",
    "    raise Exception(f\"you need to run the {org_t_d} notebook\")\n",
    "shutil.copytree(org_t_d,ies_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us quickly load the Pst control file and remind ourselves of observations and predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst_name = os.path.join(ies_d, \"freyberg_mf6.pst\")\n",
    "pst_freyberg = pyemu.Pst(pst_name)\n",
    "pst_freyberg.pestpp_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pst_freyberg.pestpp_options['forecasts'].split(',')\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by loading the prior observation ensemble. Check how many realisations it has. These are our \"training data\". Let's use all of them. (if you like, you can experiment with what happens by using less realizations in the training dataset...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oe_name = pst_name.replace(\".pst\", \".0.obs.csv\")\n",
    "oe_pr = pyemu.ObservationEnsemble.from_csv(pst=pst_freyberg, filename=pst_name.replace(\".pst\", \".0.obs.csv\"))\n",
    "oe_pt = pyemu.ObservationEnsemble.from_csv(pst=pst_freyberg, filename=pst_name.replace(\".pst\", \".3.obs.csv\"))\n",
    "\n",
    "oe_pr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a previous notebooks, we set the observation weights to be \"balanced for visibility\". Thus they do not reflect a measure of measurement noise. However, we included the `standard_deviation` column in the PEST observation data section. Ensembles of observation noise are generated using the latter.\n",
    "\n",
    "Experience suggests that, when conditioning a Gaussian process model, there is not much advantage in weighting for visibility. Achieving model to measurement fits commensurate with noise is rarely an issue with these methods. In practice, it may be more convenient to assign weights explicitly as the inverse of the standard deviation of noise. This provides an easy way to verify if over-fitting is occurring (i.e Phi should be >= than the number of non-zero obs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pst_freyberg.observation_data\n",
    "obs.loc[obs.weight>0, ['obsval','weight','standard_deviation']].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change our observation weights to be the inverse of the standard deviation of noise. Alternatively we could explicitly pass in an ensemble of observation noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.loc[obs.weight>0,\"weight\"] = 1.0 / obs.loc[obs.weight>0,\"standard_deviation\"]\n",
    "assert obs.weight.sum()>0, \"no non-zero obs weights found\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from previous notebooks that we are carrying along a whole bunch of zero-weighted observations. Just to speed things up (in particular when we implement normal-score transformation later on), let's drop these observations from the PEST control file. We will only keep the observation groups that have non-zero weights and the `predictions`.  This is probably best practice for real-world DSI usecases...\n",
    "\n",
    "Note that, we don't have to do this...it just speeds things up a bit during the set up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f for f in pst_freyberg.instruction_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop obs to reduce matrix size\n",
    "drop_list = [f for f in pst_freyberg.instruction_files if f.startswith(\"hdslay\")]\n",
    "drop_list.extend([f for f in pst_freyberg.instruction_files if \".npf_k_layer1.\" in f])\n",
    "drop_list.append(\"inc.csv.ins\")\n",
    "drop_list.append(\"cum.csv.ins\")\n",
    "for o in drop_list:\n",
    "    pst_freyberg.drop_observations(os.path.join(ies_d,o),pst_path='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same for the observation ensembles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obsnmes = pst_freyberg.observation_data.obsnme.values\n",
    "\n",
    "oe_pr = oe_pr.loc[:, obsnmes]\n",
    "oe_pt = oe_pt.loc[:, obsnmes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oe_pr.shape[0], \"realizations used for DSI training\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the DSI model and PEST setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pyEMU, the \"ensemble data space\" `EnDS` class is the entry point for all things DSI. It is initialized with the PEST control file and the ensemble of model outputs. When initialized it prepares in memory the various components required for DSI and associated analyses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ends = pyemu.EnDS(pst=pst_freyberg,\n",
    "                  sim_ensemble=oe_pr,\n",
    "                  predictions=predictions,\n",
    "                  verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is where things get fancy. We need to construct a surrogate model. We also need to construct a new PEST setup to condition the surrogate. Luckily, pyEMU has a function that does all the hard work for us!\n",
    "\n",
    "When calling `.prep_for_dsi()` on the `EnDS` object, pyemu will prepare a Pst object and folder with all the files required to run and condition the surrogate model using pestpp-ies. The new PEST control file is named `dsi.pst`. It contains all the observations that were included in the Pst object passed to `EnDS`. Parameters in the new control file are the vector of $\\mathbf{x}$, i.e. the surrogate model parameters.\n",
    "\n",
    "The `.prep_for_dsi()` method provides some optional arguments that enable the user to specify whether observations should be subject to normal-score transformation (useful to improve the Gaussianity of the data - a condition on which this method relies) and which method to use for calculating the square root of the covariance matrix. If the `use_ztz` argument is set to `True`, DSI will be set up using the same approach employed by `DSI2` in the PEST utilities. The optional argument `energy` can be used to specify the level of singular value truncation. \n",
    "\n",
    "Let us start by using no normal-score transformation and the default approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_d = \"dsi_template\"\n",
    "pst_dsi = ends.prep_for_dsi(t_d=t_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One extra step for our case, because `.prep_for_dsi` does not copy over executables (note we dont need MODFLOW for DSI!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pst_template copy exe files\n",
    "found = False\n",
    "for f in os.listdir(os.path.join(ies_d)):\n",
    "    if f.startswith(\"pestpp-ies\"):\n",
    "        shutil.copy2(os.path.join(ies_d,f),os.path.join(t_d,f))\n",
    "        found = True\n",
    "if not found:\n",
    "    raise Exception(\"couldn't find pestpp-ies binary in {0}\".format(ies_d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at whats in this new folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(t_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the various components of the `dsi.pst` version 2 control file. Files that start with `dsi_` are the model emulator input and output files. \n",
    "\n",
    "The `dsi_pr_mean.csv` and `dsi_proj_mat.npy` are inputs required by the emulator. They are the prior mean observation values $\\bar{\\mathbf{d}}$ and the matrix of the square root of observation covariance matrix $\\mathbf{C}_d^{1/2}$. (see the \"intro to dsi\" notebook for more details)\n",
    "The `dsi_sim_vals.csv` file contains the emulator generated observation values using the input parameters, $\\mathbf{x}$, contained in `dsi_pars.csv`. \n",
    "The `forward_run.py` script contains code to read input files, run the emulator and record outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at how npar has changed. The number of parameters in the original Pst object is the number of parameters in the model. The number of parameters in the new Pst object is the number of principal components used in the emulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst_dsi.npar_adj, pst_freyberg.npar_adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify we have the same number of observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst_dsi.nobs, pst_freyberg.nobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst_dsi.nnz_obs, pst_freyberg.nnz_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right on! We are ready to get cracking. Let's run pestpp-ies and see what we get.\n",
    "\n",
    "#ATTENTION!\n",
    "\n",
    "As always, set the number of workers according to your resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the number of realizations and how many workers you have at your disposal, the next cell may take a while to run. Although the emulator model is super fast, we are running it many times.\n",
    "\n",
    "As always, ideally you would want as many realizations as you can afford. For the sake of the tutorial let's stick to 200. (The next cell might take 10-20min to run, depending on the number of workers.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the control file\n",
    "pst = pyemu.Pst(os.path.join(t_d,\"dsi.pst\"))\n",
    "\n",
    "# lets specify the number of realizations to use; usually this should be as many as you can afford\n",
    "num_reals = 200 #oe_pr.shape[0] #using few 'cause its a tutorial and dont want to take too long...\n",
    "\n",
    "pst.pestpp_options[\"ies_num_reals\"] = num_reals\n",
    "\n",
    "# and a options to reduce the number of lost reals and improve conditioning\n",
    "pst.pestpp_options[\"overdue_giveup_fac\"] = 1e30\n",
    "pst.pestpp_options[\"overdue_giveup_minutes\"] = 1e30\n",
    "pst.pestpp_options[\"ies_no_noise\"] = False \n",
    "pst.pestpp_options[\"ies_subset_size\"] = -10 # the more the merrier\n",
    "pst.pestpp_options[\"ies_bad_phi_sigma\"] = 2.0\n",
    "\n",
    "# set noptmax \n",
    "pst.control_data.noptmax = 3\n",
    "\n",
    "# and re-write\n",
    "pst.write(os.path.join(t_d,\"dsi.pst\"),version=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the master dir\n",
    "m_d = \"master_dsi\"\n",
    "pyemu.os_utils.start_workers(t_d,\"pestpp-ies\",\"dsi.pst\",num_workers=num_workers,worker_root='.',\n",
    "                                master_dir=m_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right on. Let's check the evolution of the objective function (remember - this is the same objective function - observations and weights- that we have been using in the other notebooks on data assimilation/history matching!). We should see it decreasing as the emulator is conditioned/trained on the observations, but we don't want it to become less than the number of nonzero-weighted observations (`nnzobs`) #overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_d = \"master_dsi\"\n",
    "phidf = pd.read_csv(os.path.join(m_d,\"dsi.phi.actual.csv\"),index_col=0)\n",
    "\n",
    "fig,ax=plt.subplots(1,1,figsize=(4,4))\n",
    "ax.plot(phidf.index,phidf['mean'],\"bo-\", label='dsi')\n",
    "\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylabel('phi')\n",
    "ax.set_xlabel('iteration')\n",
    "\n",
    "ax.text(0.7,0.9,f\"nnz_obs: {pst.nnz_obs}\\nphi_dsi: {phidf['mean'].iloc[-1]:.2f}\",\n",
    "        transform=ax.transAxes,ha=\"right\",va=\"top\")\n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as usual, we bring this back to the predictions. How has DSI performed? While we are at it, let's compare to the numerical model derived predictions we obtained in the \"freyberg ies\" notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dsi_hist(m_d,pst,iteration=3):\n",
    "    pr_oe_dsi = pd.read_csv(os.path.join(m_d,\"dsi.0.obs.csv\"),index_col=0)\n",
    "    pt_oe_dsi = pd.read_csv(os.path.join(m_d, f\"dsi.{iteration}.obs.csv\"), index_col=0)\n",
    "\n",
    "    #pv = pyemu.ObservationEnsemble(pst=pst,df=oe_pt).phi_vector\n",
    "    #pv_dsi = pyemu.ObservationEnsemble(pst=pst, df=pt_oe_dsi).phi_vector\n",
    "\n",
    "    fig,axes = plt.subplots(len(predictions),1,figsize=(7,7))\n",
    "    for p,ax in zip(predictions,axes):\n",
    "            #calculate consistent bin edges\n",
    "            bins = np.linspace(\n",
    "                    min(oe_pr.loc[:,p].values.min(),\n",
    "                        oe_pt.loc[:,p].values.min(),\n",
    "                        pr_oe_dsi.loc[:,p].values.min(),\n",
    "                        pt_oe_dsi.loc[:,p].values.min()),\n",
    "                    max(oe_pr.loc[:,p].values.max(),\n",
    "                        oe_pt.loc[:,p].values.max(),\n",
    "                        pr_oe_dsi.loc[:,p].values.max(),\n",
    "                        pt_oe_dsi.loc[:,p].values.max()),\n",
    "                    50)\n",
    "\n",
    "            ax.hist(oe_pr.loc[:,p].values,alpha=0.5,facecolor=\"0.5\",density=True,label=\"prior\",bins=bins)\n",
    "            ax.hist(oe_pt.loc[:, p].values,  alpha=0.5, facecolor=\"b\",density=True,label=\"posterior\",bins=bins)\n",
    "            ax.hist(pr_oe_dsi.loc[:, p].values,  facecolor=\"none\",hatch=\"/\",edgecolor=\"0.5\",lw=0.5,density=True,label=\"dsi prior\",bins=bins)\n",
    "            ax.hist(pt_oe_dsi.loc[:, p].values,  facecolor=\"none\",density=True,hatch=\"/\",edgecolor=\"b\",lw=.5,label=\"dsi posterior\",bins=bins)\n",
    "            \n",
    "            fval = pst.observation_data.loc[p,\"obsval\"]\n",
    "            ax.plot([fval,fval],ax.get_ylim(),\"r-\",label='truth')\n",
    "            \n",
    "            ax.set_title(p,loc=\"left\")\n",
    "            ax.legend(loc=\"upper right\")\n",
    "            ax.set_yticks([])\n",
    "    fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration=phidf.loc[phidf[\"mean\"]>= pst.nnz_obs].index.values[-1]\n",
    "plot_dsi_hist(m_d,pst,iteration=iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too shabby. Prediction variance has decreased for most forecasts (blue histograms have less spread than grey ones). And the posterior distributions all capture the truth. \n",
    "\n",
    "There is a noticeable issue for the particle travel time prediction: the model emulator is predicting physically impossible values (i.e. negative times). On top of that, the physics-based model prior and posterior show a distinctly skewed distribution, whilst the emulator derived output are symmetric. This is due to the Gaussian assumption of the emulator. To avoid it, we can (and should!) apply a normal-score transformation of the data. stay tuned! (Alternatively, we could also/or apply log-transformation to the `part_time` values.)\n",
    "\n",
    "First, lets look at another common issue: saw-tooth patterns in time-series outputs. Make a quick plot of the future time series outputs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tseries_ensembles(pt_oe, onames=[\"hds\",\"sfr\"]):\n",
    "    pst.try_parse_name_metadata()\n",
    "    # get the observation data from the control file and select \n",
    "    obs = pst.observation_data.copy()\n",
    "    # onames provided in oname argument\n",
    "    obs = obs.loc[obs.oname.apply(lambda x: x in onames)]\n",
    "    # only non-zero observations\n",
    "    obs = obs.loc[obs.obgnme.apply(lambda x: x in pst.nnz_obs_groups),:]\n",
    "    # make a plot\n",
    "    ogs = obs.obgnme.unique()\n",
    "    fig,axes = plt.subplots(len(ogs),1,figsize=(7,2*len(ogs)))\n",
    "    ogs.sort()\n",
    "    # for each observation group (i.e. timeseries)\n",
    "    for ax,og in zip(axes,ogs):\n",
    "        # get values for x axis\n",
    "        oobs = obs.loc[obs.obgnme==og,:].copy()\n",
    "        oobs.loc[:,\"time\"] = oobs.time.astype(float)\n",
    "        oobs.sort_values(by=\"time\",inplace=True)\n",
    "        tvals = oobs.time.values\n",
    "        onames = oobs.obsnme.values\n",
    "        ax.plot(oobs.time,oobs.obsval,color=\"fuchsia\",lw=2,zorder=100)\n",
    "        # plot prior\n",
    "        #[ax.plot(tvals,pr_oe.loc[i,onames].values,\"0.5\",lw=0.5,alpha=0.5) for i in pr_oe.index]\n",
    "        # plot posterior\n",
    "        [ax.plot(tvals,pt_oe.loc[i,onames].values,\"b\",lw=0.5,alpha=0.5) for i in pt_oe.index]\n",
    "        # plot measured+noise \n",
    "        oobs = oobs.loc[oobs.weight>0,:]\n",
    "        tvals = oobs.time.values\n",
    "        onames = oobs.obsnme.values\n",
    "        #[ax.plot(tvals,noise.loc[i,onames].values,\"r\",lw=0.5,alpha=0.5,zorder=0) for i in noise.index]\n",
    "        ax.plot(oobs.time,oobs.obsval,\"r-o\",lw=2,zorder=100)\n",
    "        ax.set_title(og,loc=\"left\")\n",
    "    fig.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell plots the posterior of three forecast time-series. As you can see, DSI generated outputs are nice and smooth in the historical period. However, the same cannot be said for the forecast period, where a \"saw-toothed\" behaviour can be seen, with the time-series jumping up and down. It is specially evident in the early-future for site `trgw-0-3-8`. This may also be somewhat mitigated by using normal-score transformation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_oe_dsi = pd.read_csv(os.path.join(m_d, f\"dsi.{iteration}.obs.csv\"), index_col=0)\n",
    "fig = plot_tseries_ensembles(pt_oe_dsi, onames=[\"hds\",\"sfr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DSI with normal-score transformation\n",
    "\n",
    "We will now repeat all of the above, but using the normal-score transformation (NST) option when calling `.prep_for_dsi()`. This option will first apply a normal-score transform to all model derived outputs in the training dataset. Transformed observation values will have a distribution which is more similar to Gaussian, thus better respecting the assumptions on which DSI relies. Then, during the DSI forward run, emulator-derived outputs are back-transformed into the original data space, so as to allow for direct comparison to measured values.\n",
    "\n",
    "In the `pyemu` python implementation this comes at a slight cost of increased run time, both in setting up the emulator, as well as in the forward run. The latter may cost a couple of seconds more. Sorry about that. \n",
    "\n",
    "(The next cell might take 10-20min to run, depending on the number of workers.)\n",
    "\n",
    "Here we go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_d = \"dsi_template\"\n",
    "\n",
    "ends = pyemu.EnDS(pst=pst_freyberg,\n",
    "                  sim_ensemble=oe_pr,\n",
    "                  predictions=predictions,\n",
    "                  verbose=False)\n",
    "\n",
    "pst_dsi = ends.prep_for_dsi(t_d=t_d,\n",
    "                            apply_normal_score_transform=True,\n",
    "                            nst_extrap=\"quadratic\",\n",
    "                            )\n",
    "\n",
    "# from pst_template copy exe files\n",
    "found = False\n",
    "for f in os.listdir(os.path.join(ies_d)):\n",
    "    if f.startswith(\"pestpp-ies\"):\n",
    "        shutil.copy2(os.path.join(ies_d,f),os.path.join(t_d,f))\n",
    "        found = True\n",
    "if not found:\n",
    "    raise Exception(\"couldnt find pestpp-ies binary in {0}\".format(ies_d))\n",
    "\n",
    "# load the control file\n",
    "pst = pyemu.Pst(os.path.join(t_d,\"dsi.pst\"))\n",
    "\n",
    "# lets specify the number of realizations to use; as usual this should be as many as you can afford\n",
    "pst.pestpp_options[\"ies_num_reals\"] = num_reals\n",
    "\n",
    "# and a options to reduce the number of lost reals and improve conditioning\n",
    "pst.pestpp_options[\"overdue_giveup_fac\"] = 1e30\n",
    "pst.pestpp_options[\"overdue_giveup_minutes\"] = 1e30\n",
    "pst.pestpp_options[\"ies_no_noise\"] = False \n",
    "pst.pestpp_options[\"ies_subset_size\"] = -10\n",
    "pst.pestpp_options[\"ies_bad_phi_sigma\"] = 2.0\n",
    "\n",
    "\n",
    "# set noptmax \n",
    "pst.control_data.noptmax = 3\n",
    "\n",
    "# and re-write\n",
    "pst.write(os.path.join(t_d,\"dsi.pst\"),version=2)\n",
    "\n",
    "# the master dir\n",
    "m_d = \"master_dsi_nst\"\n",
    "pyemu.os_utils.start_workers(t_d,\"pestpp-ies\",\"dsi.pst\",num_workers=num_workers,worker_root='.',\n",
    "                                master_dir=m_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phidf = pd.read_csv(os.path.join(m_d,\"dsi.phi.actual.csv\"),index_col=0)\n",
    "fig,ax=plt.subplots(1,1,figsize=(4,4))\n",
    "ax.plot(phidf.index,phidf['mean'],\"bo-\", label='dsi')\n",
    "\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylabel('phi')\n",
    "ax.set_xlabel('iteration')\n",
    "\n",
    "ax.text(0.7,0.9,f\"nnz_obs: {pst.nnz_obs}\\nphi_dsi: {phidf['mean'].iloc[-1]:.2f}\",\n",
    "        transform=ax.transAxes,ha=\"right\",va=\"top\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we achieved near noise levels of fit without too much effort. We would not want to fit any further...and we might even wish to not fit this well...But lets take the results of the last iteration and see how our predictions did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration=phidf.loc[phidf[\"mean\"]>= pst.nnz_obs].index.values[-1]\n",
    "plot_dsi_hist(m_d,pst,iteration=iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet! #winning. This time around, the DSI prior looks a lot more similar to those we obtained with the Freyberg model. On top of that, we now see the distribution of predicted `particle time` no longer has physically infeasible values, and it follows the same log distribution with a long tail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final remarks\n",
    "\n",
    "Data space inversion provides a powerful tool for data assimilation and uncertainty quantification for cases in which model complexity and/or computational cost preclude the use of traditional methods. \n",
    "\n",
    " - use as many reals as possible when generating training data. The more the better.\n",
    " - the same applies for DSI conditioning with IES. The more reals the better.\n",
    "  - make sure the DSI generated prior reflects the physical model based prior. This may require a number of DSI realizations equal to or greater than the training data. Specially if using normal-score transformation.\n",
    " - if forecasts are leaning towards the ends (or beyond!) of the prior distribution, this is a red flag. It is probably worth revising the physics-based model prior parameter distributions and re-training the emulator.\n",
    " - don't overfit...#duh\n",
    "\n",
    " ### Benefits\n",
    " - extreme numerical efficiency. We only need a few hundred runs of the physics-based model as we are not using it for parameter adjustment. It is only used to construct/train the statistical model.\n",
    " - can use DSI as a verification of IES forecasts: are these too wide and/or not wide enough?\n",
    " - allows for parameter distributions of arbitrary complexity in the prior.\n",
    " - can be used for analyzing the worth of existing and as-of-yet uncollected data to reduce predictive uncertainty. With no assumption of linearity! (see next notebook)\n",
    "\n",
    "### Drawbacks\n",
    " - if the relationship between the past and future are highly non-linear, it will not work - DSI is predicated on the assumed linearity between model outputs from the past and model outputs from the future\n",
    " - it is not possible to view the process-based model (eg MODFLOW) inputs that produce the DSI forecast posterior distribution, so can't \"see\" what model inputs might be causing extreme results.\n",
    " - when simulating future scenarios, you have to rerun the training ensemble through the process-based model and also rerun the DSI training process for each scenario. But you would also have to run the ensemble for each scenario with the physics-based model..."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
