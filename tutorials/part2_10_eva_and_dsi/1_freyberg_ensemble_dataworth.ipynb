{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt;\n",
    "import psutil\n",
    "\n",
    "import sys\n",
    "import pyemu\n",
    "import flopy\n",
    "assert \"dependencies\" in flopy.__file__\n",
    "assert \"dependencies\" in pyemu.__file__\n",
    "sys.path.insert(0,\"..\")\n",
    "import herebedragons as hbd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Variance Analysis (EVA)\n",
    "\n",
    "EVA is a method that allows assessment of the worth of data in reducing forecast uncertainty [He et al (2018)](https://doi.org/10.2118/182609-PA), much like FOSM-based methods of data worth analysis discussed in other tutorial notebooks.  However, EVA has a major advantage: it relies on ensembles instead of finite difference derivatives and is independent of the number of model parameters.  In this way, EVA has a lot in common with DSI...\n",
    "\n",
    "Based on a multivariate Gaussian assumption between the observation data and the objective function, EVA measures the expected reduction in uncertainty using covariance information estimated from a group of simulations. The method has several innovative features compared to other approaches. \n",
    "\n",
    " - Firstly, the EVA method copes well with high parameter dimensionality and nonlinearity between model inputs/parameters and model outputs, allowing it to handle nonlinear forward models and any number of parameters. Much as we discussed for model-emulator history matching with DSI, EVA can be undertaken on models of arbitrary complexity and parameterization...all you need is an ensemble of model outputs! \n",
    "\n",
    " - Secondly, even if the multivariate Gaussian assumption between the historic observations and the forecasts quantities of interest is violated, the EVA method still provides a lower bound on the expected uncertainty reduction, which can be useful for giving a conservative estimate of the performance of the data collection programs. \n",
    "\n",
    " - Finally, EVA also gives an estimate of the shift in the mean of the foreacast posterior distribution, which is crucial for calculating value-of-information (VOI). This enables exploration of what the forecast mean is expected to be, given a set of assumed observed values. Powerful stuff!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Admin\n",
    "As usual, let's get some files from previous tutorials..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the temporary working folder\n",
    "t_d = os.path.join('master_ies_1a')\n",
    "if os.path.exists(t_d):\n",
    "    shutil.rmtree(t_d)\n",
    "\n",
    "org_t_d = os.path.join(\"..\",\"part2_06_ies\",\"master_ies_1a\")\n",
    "if not os.path.exists(org_t_d):\n",
    "    raise Exception(f\"you need to run the {org_t_d} notebook\")\n",
    "shutil.copytree(org_t_d,t_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting ready\n",
    "\n",
    "As for FOSM, we need a few ingredients for EVA:\n",
    " \n",
    "- we need a pst control file with observation data\n",
    "- we need a list of `predictions` obsnmes\n",
    "- we need an ensemble of observation values (this can be prior or posterior...which one to use depends on what one is trying to accomplish...)\n",
    "- we need an ensemble of noise (pre-prepared or obtained from the `weight` or `standard_deviation` columns in the `pst.observation_data`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Pst\n",
    "pst_name = os.path.join(t_d, \"freyberg_mf6.pst\")\n",
    "pst = pyemu.Pst(pst_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "predictions = pst.pestpp_options['forecasts'].split(',')\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the prior obs ensemble\n",
    "oe_pr = pyemu.ObservationEnsemble.from_csv(pst=pst, filename=pst_name.replace(\".pst\", \".0.obs.csv\"))\n",
    "# the posterior obs ensemble\n",
    "oe_pt = pyemu.ObservationEnsemble.from_csv(pst=pst, filename=pst_name.replace(\".pst\", \".3.obs.csv\"))\n",
    "\n",
    "# note how oe_pt may have lost a few realizations along the way\n",
    "oe_pr.shape, oe_pt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the noise ensemble. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in an obs+noise ensemble\n",
    "noise = pyemu.ObservationEnsemble.from_csv(pst,pst_name.replace(\".pst\", \".obs+noise.csv\"))\n",
    "noise.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate value of existing obs\n",
    "\n",
    "For starters, let's look at the data worth of observations for which we already have data. (i.e. the observations we previously used for history matching with pestpp-ies). \n",
    "\n",
    "In the `pyemu` world, all things EVA/DSI start with the `EnDS` Class. Spin one up now. Note that, as we want to evaluate the value of \"existing\" observations, with which we might be going to use to history match, we will use the prior ensemble:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ends = pyemu.EnDS(pst=pst,\n",
    "                  sim_ensemble=oe_pr, #prior\n",
    "                  noise_ensemble=noise,\n",
    "                  predictions=predictions,\n",
    "                  verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that, for the Freyberg model, our observations are time-series of values recorded at monitored sites. Let's evaluate the worth of each time-series, rather than each observation independently. Create a dictionary with obs group name (i.e., time-series) as the keys, and the respective observation names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pst.observation_data\n",
    "nzobs= obs.loc[pst.nnz_obs_names].copy()\n",
    "obslist_dict={}\n",
    "for o in nzobs.obgnme.unique():\n",
    "    obslist_dict[o] = nzobs.loc[nzobs.obgnme==o].obsnme.tolist()\n",
    "obslist_dict.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "right on! we are good to go...let's kick this off. \n",
    "\n",
    "First, let's assess the worth of these observations in conditioning the prior (`oe_pr`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_dfs,dfstd,dfpercen = ends.get_posterior_prediction_moments(obslist_dict=obslist_dict.copy(),\n",
    "                                                                sim_ensemble=oe_pr, #if None, uses the sim_ensemble originally passed to ends\n",
    "                                                                include_first_moment=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dfstd` is a dataframe of the expected standard deviation after conditioning with the respective observations.\n",
    "\n",
    "`dfpercen` is the \"percetage reduction\" in uncertainty. This allows for easier comparison between predictions of different magnitudes.\n",
    "\n",
    "Here goes a quick and dirty plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = dfpercen.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each entry on the x-axis is one of the observation sites with time-series of measured data...+ the `posterior`, which shows the data worth of all observations concurrently. The y-axis shows percentage uncertainty reduction. Each bar is for one of the four predictions.  \n",
    "\n",
    "The larger the bar, the more worth that observation group has when reducing uncertainty for the respective prediction. \n",
    "\n",
    "Now the same, but using the posterior observation ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_dfs,dfstd,dfpercen = ends.get_posterior_prediction_moments(obslist_dict=obslist_dict.copy(),\n",
    "                                                                sim_ensemble=oe_pt, #if None, uses the sim_ensemble originally passed to ends\n",
    "                                                                include_first_moment=False)\n",
    "dfpercen.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you may be asking: \"Hold up! How come we are expecting to have more uncertainty reduction??? Didn't we already history match against these obs??\"\n",
    "\n",
    "Totally right. However, here we are evaluating the worth assuming we get a fit that is *commensurate with measurement noise*! (Which pestpp-ies didn't). What this is telling us is that, if we were able to get a better fit, we could reduce uncertainty of these predictions by this much further. (Assuming we avoid inducing bias, etc etc). Based on these outcomes one might determine whether or not it is worth revisiting model construction/parameterization/history matching..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the worth of new data\n",
    "\n",
    "We can do the same thing for \"as of yet uncollected data\". The procedure is the same.\n",
    "\n",
    "First, a dictionary of obsname groups for time-series of data collected from potential new sites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pst.observation_data\n",
    "\n",
    "# get obs from the historic period\n",
    "historytimes = obs.loc[pst.nnz_obs_names].time.tolist()\n",
    "nnzobgnmes = obs.loc[(obs.oname=='hds') & (obs.weight>0)].obgnme.unique()\n",
    "obgnmes = obs.loc[(~obs.obgnme.isin(nnzobgnmes)) &\n",
    "                (obs.oname=='hds')\n",
    "                ].obgnme.unique()\n",
    "# group by time series\n",
    "obslist_dict = {o:obs.loc[(obs.obgnme==o) &\n",
    "                          (obs.time.isin(historytimes))\n",
    "                          ].obsnme.tolist() for o in obgnmes}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: because these observations were not previously assigned a weight when generating the obs+noise ensemble, we need to make sure that we assign appropriate weight and/or standard_deviation values in the Pst control file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for o in list(obslist_dict.keys()):\n",
    "    obs.loc[obslist_dict[o], \"weight\"] = 1/0.1\n",
    "    obs.loc[obslist_dict[o], \"standard_deviation\"] = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we go..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ends = pyemu.EnDS(pst=pst,\n",
    "                  sim_ensemble=oe_pt, #for the posterior\n",
    "                  #noise_ensemble=noise, #NOTE: this will now be generated from the observation_data\n",
    "                  predictions=predictions,\n",
    "                  verbose=False)\n",
    "\n",
    "mean_dfs,dfstd,dfpercen = ends.get_posterior_prediction_moments(obslist_dict=obslist_dict.copy(),\n",
    "                                                                sim_ensemble=oe_pt,\n",
    "                                                                include_first_moment=False)\n",
    "dfpercen.plot(kind='bar')                                                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and there we have it: the % uncertainty reduction for each forecast for each of the potential new observation points...as well as the total expected uncertainty reduction if all observations are included as non-zero weight observations. (...assuming a fit commensurate to noise yadda yadda...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate whether uncertainty has converged\n",
    "\n",
    "Another useful application is to test whether uncertainty has converged...i.e. do we have enough realizations?\n",
    "\n",
    "We can accomplish this by repeatedly running `ends.get_predictive_posterior_moments()`, with less than all the possible realizations, and checking if the uncertainty estimates change.\n",
    "\n",
    "`ends.get_posterior_prediction_convergence_summary()` automates this process. All we need to do is pass a sequence of \"number of realizations\" to test (`num_realization_sequence`), as well as a list of how many randomly selected realizations (`num_replicate_sequence`) to replicate each of the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the sequence of number of realizations to test\n",
    "num_reals_seq = [int(i) for i in np.linspace(10,oe_pt.shape[0],20)]\n",
    "num_reals_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first we will test with 10 reals, then 54, then 98 and so on...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many randomly slected reals to sample, for each of the elements in the `num_real_seq` list\n",
    "num_reps_seq = len(num_reals_seq)*[20]\n",
    "num_reps_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first, we will randomly select 10 reals and test them, then repeat 5 times.\n",
    "\n",
    "then, we will randomly select 54 reals and test them, then repeat 5 times.\n",
    "\n",
    "and so on...\n",
    "\n",
    "\n",
    "...right, here we go. This may take a few minutes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pst.observation_data\n",
    "nzobs= obs.loc[pst.nnz_obs_names].copy()\n",
    "obslist_dict={}\n",
    "for o in nzobs.obgnme.unique():\n",
    "    obslist_dict[o] = nzobs.loc[nzobs.obgnme==o].obsnme.tolist()\n",
    "obslist_dict.items()\n",
    "ends = pyemu.EnDS(pst=pst,\n",
    "                  sim_ensemble=oe_pt,\n",
    "                  noise_ensemble=noise,\n",
    "                  predictions=predictions,\n",
    "                  verbose=False)\n",
    "\n",
    "means = ends.get_posterior_prediction_convergence_summary(\n",
    "                                                        num_realization_sequence=num_reals_seq,\n",
    "                                                        num_replicate_sequence=num_reps_seq,\n",
    "                                                        obslist_dict=[])#obslist_dict.copy() #important to pass a copy, as it gets changed in the background\n",
    "                                                        #)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now to plot those results up..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(2,2,figsize=(8,8),sharex=True)\n",
    "axs = axs.flatten()\n",
    "for e,p in enumerate(predictions):\n",
    "    ax = axs[e]\n",
    "    ax.set_title(p.split(\"usecol:\")[-1])\n",
    "    ax.set_ylabel(\"mean predictive standard deviation\")\n",
    "    ax.set_xlabel(\"num reals\")\n",
    "\n",
    "    for k in means.keys():\n",
    "        df = means[k]\n",
    "        value = df.loc[\"posterior\",p]\n",
    "        ax.scatter(k,value,color='b',s=19)\n",
    "    ax.grid(lw=1)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet, there we have it. The uncertainty convergence for each prediction versus number of reals. Where the plots plateau indicates uncertainty has converged. For most of the predictions, approximately 200 reals should be sufficient. As usual, `part_time` is a bit more challenging. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
